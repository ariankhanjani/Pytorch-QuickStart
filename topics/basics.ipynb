{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Topics covered in this notebook:\n",
        "\n",
        "* ### Introduction to Tensors — What tensors are and why they’re core to PyTorch.\n",
        "* ### Creating Tensors — Using torch.tensor(), torch.zeros(), torch.rand(), and more.\n",
        "* ### Inspecting Tensor Attributes — Shape, dtype, device, and dimensionality.\n",
        "* ### Indexing and Slicing — Accessing and modifying tensor elements.\n",
        "* ### Reshaping and Transposing — Using .view(), .reshape(), .unsqueeze(), .permute().\n",
        "* ### Tensor Operations — Element-wise math, broadcasting, reductions, and matrix multiplications.\n",
        "* ### In-place Operations — Understanding the difference between add() and add_().\n",
        "* ### Type Casting and Conversions — Changing data types and converting between NumPy arrays.\n",
        "* ### Device Management — Moving tensors between CPU and GPU.\n",
        "* ### Reproducibility — Setting random seeds for consistent results.\n",
        "* ### Mini Exercises — Hands-on tensor manipulation and basic math challenges.\n",
        "* ### Summary & Key Takeaways — Recap of what you learned.\n",
        "\n",
        "## The goal is Get comfortable creating, inspecting, and manipulating PyTorch tensors — the building blocks for everything else."
      ],
      "metadata": {
        "id": "CPS9WzK_gwTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Imports & Setup (run first)"
      ],
      "metadata": {
        "id": "c9_PdPgVkioj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8Nz9XGufYK8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic checks\n",
        "print(f'Pytorch version: {torch.__version__}')\n",
        "print(f'CUDA Available: {torch.cuda.is_available()}')\n",
        "device  =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLdENStRi1ve",
        "outputId": "23d8166c-34ac-4f18-a749-0d70b1cffc64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version: 2.8.0+cu126\n",
            "CUDA Available: False\n",
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Introduction to Tensors (tiny demo)"
      ],
      "metadata": {
        "id": "XE-ym6vt2EiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A tensor is a multi-dimensional array (like NumPy), but can live on GPU and support autograd.\n",
        "scalar = torch.tensor(3.14)                   # 0-D\n",
        "vector = torch.tensor([1.0, 2.0, 3.0])        # 1-D\n",
        "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]]) #2-D\n",
        "\n",
        "print(\"Scalar\", scalar, \"shape:\", scalar.shape)\n",
        "print(\"Vector\", vector, \"shape:\", vector.shape)\n",
        "print(\"Matrix\", matrix, \"shape:\", matrix.shape)"
      ],
      "metadata": {
        "id": "Q3gDzVomi5gO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04727d7c-0943-4cfb-83f5-9bd26b33d3e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scalar tensor(3.1400) shape: torch.Size([])\n",
            "Vector tensor([1., 2., 3.]) shape: torch.Size([3])\n",
            "Matrix tensor([[1, 2, 3],\n",
            "        [4, 5, 6]]) shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Creating Tensors (factory functions and from lists/NumPy)"
      ],
      "metadata": {
        "id": "8ipl9s454bFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor creation examples\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "zeros = torch.zeros((2, 3))\n",
        "ones = torch.ones((2, 2))\n",
        "rand_u = torch.rand((2, 2))     # uniform [0,1)\n",
        "rand_n = torch.randn((2, 2))    # normal (mean=0, std=1)\n",
        "ar = torch.arange(0, 10, 2)     # like numpy.arange\n",
        "lin = torch.linspace(0, 1, 5)   # linearly spaced\n",
        "\n",
        "np_arr = np.array([2, 4])\n",
        "to_tensor = torch.from_numpy(np_arr)    # from NumPy (shares memory)\n",
        "\n",
        "print(\"a:\", a)\n",
        "print(\"zeros:\", zeros)\n",
        "print(\"rand_n mean/std:\", rand_n.mean().item(), rand_n.std().item())\n",
        "print(\"from numpy:\", to_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEJPDdLx4B0u",
        "outputId": "413faafa-e9db-496e-c1ba-e88c448ad829"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([1., 2., 3.])\n",
            "zeros: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "rand_n mean/std: -0.08813610672950745 1.6108542680740356\n",
            "from numpy: tensor([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Inspecting Tensor Attributes"
      ],
      "metadata": {
        "id": "h4cmqZerC9mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attributes & inspection\n",
        "t = torch.randn(2, 3, dtype=torch.float32, device=device)\n",
        "\n",
        "print(\"shape:\", t.shape)\n",
        "print(\"ndim:\", t.ndim)\n",
        "print(\"dtype:\", t.dtype)\n",
        "print(\"device:\", t.device)\n",
        "print(\"numel (total elements):\", t.numel())\n",
        "\n",
        "# scalar.item()\n",
        "scalar = torch.tensor(7.0)\n",
        "print(\"scalar.item():\", scalar.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11GG2ZO4wOC",
        "outputId": "7442ce34-6e48-45f3-8182-09265325906f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([2, 3])\n",
            "ndim: 2\n",
            "dtype: torch.float32\n",
            "device: cpu\n",
            "numel (total elements): 6\n",
            "scalar.item(): 7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Indexing and Slicing"
      ],
      "metadata": {
        "id": "YmkJj7CaD_lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing and slicing\n",
        "x = torch.arange(16).reshape(4, 4)\n",
        "print(\"x:\\n\", x)\n",
        "\n",
        "print(\"first row:\", x[0] )\n",
        "print(\"second column:\", x[:,1] )\n",
        "print(\"last two rows:\", x[-2:] )\n",
        "print(\"slice with step:\", x[ ::2, :: 2] )   # every 2nd row/col\n",
        "\n",
        "# boolean/fancy indexing\n",
        "mask = x % 2 == 0\n",
        "print(\"even elements:\", x[mask])\n",
        "\n",
        "indices = torch.tensor([0, 2])\n",
        "print(\"rows 0 and 2:\", x[indices])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li4_nZcWDLeQ",
        "outputId": "f5627f7f-1c87-4199-b0ce-10dfdbed0af8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:\n",
            " tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11],\n",
            "        [12, 13, 14, 15]])\n",
            "first row: tensor([0, 1, 2, 3])\n",
            "second column: tensor([ 1,  5,  9, 13])\n",
            "last two rows: tensor([[ 8,  9, 10, 11],\n",
            "        [12, 13, 14, 15]])\n",
            "slice with step: tensor([[ 0,  2],\n",
            "        [ 8, 10]])\n",
            "even elements: tensor([ 0,  2,  4,  6,  8, 10, 12, 14])\n",
            "rows 0 and 2: tensor([[ 0,  1,  2,  3],\n",
            "        [ 8,  9, 10, 11]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Reshaping & Transposing"
      ],
      "metadata": {
        "id": "k4SpiZwqMUv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape, view, unsqueeze, squeeze, transpose, permute\n",
        "y = torch.arange(12)\n",
        "print(\"y:\", y)\n",
        "\n",
        "y2 = y.reshape(3, 4)      # new view (may copy or not)\n",
        "y3 = y.view(3, 4)         # .view requires contiguous tensor\n",
        "print(\"reshape:\", y2.shape, \"view:\", y3.shape)\n",
        "\n",
        "# add/remove dimensions\n",
        "a = torch.rand(4)\n",
        "a_unsq = a.unsqueeze(0)   # shape (1,4)\n",
        "a_squeezed = a_unsq.squeeze(0)\n",
        "print(\"unsqueezed:\", a_unsq.shape, \"squeezed:\", a_squeezed.shape)\n",
        "\n",
        "# transpose / permute\n",
        "m = torch.arange(6).reshape(2, 3)\n",
        "print(\"m:\", m)\n",
        "print(\"m.T:\", m.T)\n",
        "\n",
        "big = torch.randn(2, 3, 4)\n",
        "print(\"permute 0,2,1 shape:\", big.permute(0,2,1).shape)"
      ],
      "metadata": {
        "id": "AOBk_RmQQ8Cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e4bc12-b79a-4814-c767-8f88f09deb4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "reshape: torch.Size([3, 4]) view: torch.Size([3, 4])\n",
            "unsqueezed: torch.Size([1, 4]) squeezed: torch.Size([4])\n",
            "m: tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "m.T: tensor([[0, 3],\n",
            "        [1, 4],\n",
            "        [2, 5]])\n",
            "permute 0,2,1 shape: torch.Size([2, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "big.permute(2,0,1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpMAn5gqt8x_",
        "outputId": "de062118-01df-4777-e9af-21c55b8602f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Tensor Operations (arithmetic, reductions, matmul)"
      ],
      "metadata": {
        "id": "C6pYAS27vn9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# arithmetic & reductions\n",
        "a = torch.tensor([[1.,2.],[3.,4.]])       # a = [1, 2]  b= [10, 20]\n",
        "b = torch.tensor([[10.,20.],[30.,40.]])   #     [3, 4]     [30, 40]\n",
        "print(\"a + b =\\n\", a + b)\n",
        "print(\"a * b (elementwise) =\\n\", a * b)\n",
        "print(\"a @ b (matrix mult) =\\n\", a @ b)\n",
        "\n",
        "# broadcasting\n",
        "v = torch.tensor([1.0, 2.0])\n",
        "M = torch.ones(2,2)\n",
        "print(\"broadcast add:\", M + v)    # v broadcast across rows\n",
        "\n",
        "# reductions\n",
        "x = torch.arange(1,7).float().reshape(2,3)\n",
        "print(\"sum:\", x.sum(), \"mean:\", x.mean(), \"max:\", x.max(), \"argmax:\", x.argmax())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTBRAnJ6ujtP",
        "outputId": "cc95fdae-4480-422d-da90-bb71d67c83f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a + b =\n",
            " tensor([[11., 22.],\n",
            "        [33., 44.]])\n",
            "a * b (elementwise) =\n",
            " tensor([[ 10.,  40.],\n",
            "        [ 90., 160.]])\n",
            "a @ b (matrix mult) =\n",
            " tensor([[ 70., 100.],\n",
            "        [150., 220.]])\n",
            "broadcast add: tensor([[2., 3.],\n",
            "        [2., 3.]])\n",
            "sum: tensor(21.) mean: tensor(3.5000) max: tensor(6.) argmax: tensor(5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. In-place vs Out-of-place Operations"
      ],
      "metadata": {
        "id": "rAV3BPx5y4_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in-place vs out-of-place\n",
        "z = torch.ones(3)\n",
        "print(\"z:\", z)\n",
        "\n",
        "z2 = z + 5      #out-of-place: creates new tensor\n",
        "print(\"after z + 5 -> z remains:\", z)\n",
        "\n",
        "z.add_(10)      # in-place: modifies z\n",
        "print(\"after z.add_(10):\", z)\n",
        "\n",
        "# WARNING: in-place ops can break autograd if used incorrectly.\n",
        "# As a rule: avoid in-place ops on tensors that require gradients.\n",
        "t = torch.tensor([1.,2.,3.], requires_grad=True)\n",
        "# t.add_(1.0)     # uncommenting can cause autograd issues — use t = t + 1 instead"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-qeSSoev1LE",
        "outputId": "098105f0-7028-4ab2-f588-69e60d79a2df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z: tensor([1., 1., 1.])\n",
            "after z + 5 -> z remains: tensor([1., 1., 1.])\n",
            "after z.add_(10): tensor([11., 11., 11.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Type Casting & Conversions (NumPy interoperability)"
      ],
      "metadata": {
        "id": "zwKwbtfVzn49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dtype conversions and NumPy sharing\n",
        "a = np.array([1, 2, 3], dtype=np.int64)\n",
        "t_from_np = torch.from_numpy(a)     # shares memory with numpy array\n",
        "print(\"before change, np[0]:\", a[0], \"torch[0]:\", t_from_np[0])\n",
        "\n",
        "# mutate tensor and observe numpy change (shared memory)\n",
        "t_from_np[0] = 99\n",
        "print(\"after change, np[0]:\", a[0], \"torch[0]:\", t_from_np[0])\n",
        "\n",
        "# explicit casts\n",
        "tf = t_from_np.float()\n",
        "print(\"cast to float dtype:\", tf.dtype)\n",
        "\n",
        "# convert back to numpy (this returns a numpy array)\n",
        "back_to_np = tf.numpy()\n",
        "print(\"back_to_np dtype:\", back_to_np.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfqHD0juxc53",
        "outputId": "b545ef69-41ee-4f43-ae8a-370851351578"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before change, np[0]: 1 torch[0]: tensor(1)\n",
            "after change, np[0]: 99 torch[0]: tensor(99)\n",
            "cast to float dtype: torch.float32\n",
            "back_to_np dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Device Management (CPU ↔ GPU)"
      ],
      "metadata": {
        "id": "1rZL1RUU1Dc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: moving tensors between devices\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu_tensor = torch.randn(2,2)\n",
        "gpu_tensor = cpu_tensor.to(device)    # if CUDA unavailable, same as cpu\n",
        "\n",
        "print(\"cpu_tensor device:\", cpu_tensor.device)\n",
        "print(\"gpu_tensor device:\", gpu_tensor.device)\n",
        "\n",
        "# operations require same-device tensors\n",
        "try:\n",
        "    _ = cpu_tensor + gpu_tensor\n",
        "except Exception as e:\n",
        "    print(\"Mixing devices error (expected):\", e)\n",
        "\n",
        "# recommended pattern for models and data:\n",
        "# model.to(device); inputs = inputs.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFfzsn-50TKU",
        "outputId": "76b2b1e5-2f61-4d0f-9fa7-2ff421a06c92"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu_tensor device: cpu\n",
            "gpu_tensor device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Random Seeds & Reproducibility"
      ],
      "metadata": {
        "id": "m5L_r2hr1Y1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: reproducibility demo\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)  # if you have CUDA\n",
        "a = torch.randn(3,3)\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(3,3)\n",
        "print(\"a equals b? \", torch.allclose(a, b))  # should be True\n",
        "\n",
        "# Note: for full determinism there are extra steps and tradeoffs (CuDNN, etc.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPeZ_UzN0sFy",
        "outputId": "32dfbb70-eff7-422d-9f2f-1a2a13de1d0e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a equals b?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Mini Exercises (starter code + asserts)"
      ],
      "metadata": {
        "id": "bwCvqvON16fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: mini exercises - fill in the functions below and run tests\n",
        "\n",
        "def normalize_columns(tensor):\n",
        "    \"\"\"\n",
        "    Given a 2D tensor, normalize each column to zero mean and unit variance.\n",
        "    Return the normalized tensor.\n",
        "    \"\"\"\n",
        "    assert tensor.ndim == 2, \"tensor must be 2D\"\n",
        "    mean = tensor.mean(dim=0, keepdim=True)\n",
        "    std = tensor.std(dim=0, unbiased=False, keepdim=True)\n",
        "    return (tensor - mean) / (std + 1e-8)\n",
        "\n",
        "# Test 1\n",
        "t = torch.randn(100, 4)\n",
        "t_norm = normalize_columns(t)\n",
        "col_means = t_norm.mean(dim=0)\n",
        "col_stds = t_norm.std(dim=0, unbiased=False)\n",
        "print(\"means (≈0):\", col_means)\n",
        "print(\"stds (≈1):\", col_stds)\n",
        "assert torch.allclose(col_means, torch.zeros_like(col_means), atol=1e-6)\n",
        "assert torch.allclose(col_stds, torch.ones_like(col_stds), atol=1e-3)\n",
        "\n",
        "# Exercise 2: cosine similarity between two vectors\n",
        "def cosine_similarity(u, v):\n",
        "    # return scalar cosine similarity (u·v) / (||u|| ||v||)\n",
        "    dot = (u * v).sum()\n",
        "    un = u.norm()\n",
        "    vn = v.norm()\n",
        "    return dot / (un * vn + 1e-8)\n",
        "\n",
        "u = torch.randn(10)\n",
        "v = torch.randn(10)\n",
        "print(\"cosine similarity:\", cosine_similarity(u, v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vi5pz4i1uRU",
        "outputId": "c1e096da-b962-4259-a313-8459c18f394f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "means (≈0): tensor([-1.6689e-08,  0.0000e+00,  0.0000e+00, -7.1526e-09])\n",
            "stds (≈1): tensor([1., 1., 1., 1.])\n",
            "cosine similarity: tensor(-0.4266)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Summary"
      ],
      "metadata": {
        "id": "mz5qgEqd2Eau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary — Key takeaways\n",
        "- Tensors are the primary data structure in PyTorch (NumPy-like but GPU- and autograd-aware).\n",
        "- Use factory functions (`zeros`, `randn`, `arange`, `linspace`) to create tensors.\n",
        "- Pay attention to `.dtype`, `.shape`, `.device`.\n",
        "- Use `.to(device)` to move tensors/models to GPU.\n",
        "- In-place ops (ending with `_`) modify tensors and can break autograd — use with caution.\n",
        "- `torch.from_numpy` shares memory with NumPy arrays; modifying one affects the other.\n",
        "- Seed RNGs with `torch.manual_seed()` for repeatable experiments.\n"
      ],
      "metadata": {
        "id": "0jBJJzUI2He_"
      }
    }
  ]
}